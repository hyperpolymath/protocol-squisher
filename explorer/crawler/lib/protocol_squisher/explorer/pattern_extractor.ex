defmodule ProtocolSquisher.Explorer.PatternExtractor do
  @moduledoc """
  Extracts empirical synthesis patterns from crawler database summaries.

  Input: `summary.json` generated by `ProtocolSquisher.Explorer.EmpiricalDb`
  Output: `synthesis-hints.json` consumed by the optimizer.
  """

  @base_weights %{
    "WidenType" => 1.0,
    "MakeOptional" => 1.0,
    "AddField" => 1.0,
    "ChangeContainer" => 1.0,
    "RenameField" => 1.0
  }

  @spec extract(String.t(), String.t()) :: {:ok, map()} | {:error, term()}
  def extract(summary_path, output_path) do
    with {:ok, payload} <- File.read(summary_path),
         summary <- :json.decode(payload),
         hints <- build_hints(summary, summary_path),
         encoded <- IO.iodata_to_binary(:json.encode(hints)),
         :ok <- File.mkdir_p(Path.dirname(output_path)),
         :ok <- File.write(output_path, encoded <> "\n") do
      {:ok, Map.put(hints, "output_path", output_path)}
    else
      {:error, reason} -> {:error, reason}
    end
  end

  @spec build_hints(map(), String.t()) :: map()
  def build_hints(summary, summary_path) do
    total_records = integer(summary["total_records"])
    parse_errors = integer(summary["parse_errors"])
    corpus_errors = integer(summary["corpus_errors"])
    format_counts = map(summary["format_counts"])
    transport_counts = map(summary["transport_class_counts"])

    parse_error_rate = ratio(parse_errors, total_records)
    corpus_error_rate = ratio(corpus_errors, total_records)
    transport_distribution = normalized_distribution(transport_counts)
    format_distribution = normalized_distribution(format_counts)

    {weights, notes} =
      @base_weights
      |> tune_for_transport_distribution(transport_distribution)
      |> tune_for_errors(parse_error_rate, corpus_error_rate)
      |> tune_for_sample_size(total_records)

    %{
      "version" => 1,
      "generated_at_utc" => DateTime.utc_now() |> DateTime.to_iso8601(),
      "input_summary_path" => summary_path,
      "record_count" => total_records,
      "error_rates" => %{
        "parse" => parse_error_rate,
        "corpus" => corpus_error_rate
      },
      "transport_class_distribution" => transport_distribution,
      "format_distribution" => format_distribution,
      "top_formats" => top_counts(format_counts, 5),
      "suggestion_weights" => weights,
      "notes" => Enum.reverse(notes)
    }
  end

  defp tune_for_transport_distribution(weights, distribution) do
    wheelbarrow = Map.get(distribution, "Wheelbarrow", 0.0)
    economy = Map.get(distribution, "Economy", 0.0)

    {weights, notes} = {weights, []}

    {weights, notes} =
      if wheelbarrow >= 0.30 do
        {
          weights
          |> scale_weight("WidenType", 1.25)
          |> scale_weight("MakeOptional", 1.25)
          |> scale_weight("RenameField", 1.10),
          [
            "Wheelbarrow-heavy corpus detected; prioritize loss-reducing transforms."
            | notes
          ]
        }
      else
        {weights, notes}
      end

    if economy >= 0.25 do
      {
        weights
        |> scale_weight("ChangeContainer", 1.15)
        |> scale_weight("WidenType", 1.10),
        [
          "Economy-class conversions are common; favor container and numeric alignment."
          | notes
        ]
      }
    else
      {weights, notes}
    end
  end

  defp tune_for_errors({weights, notes}, parse_error_rate, corpus_error_rate) do
    notes =
      if parse_error_rate >= 0.10 do
        ["Parse error rate is high; treat extracted hints as lower confidence." | notes]
      else
        notes
      end

    notes =
      if corpus_error_rate >= 0.10 do
        ["Corpus analysis error rate is high; synthesis hints may be incomplete." | notes]
      else
        notes
      end

    {weights, notes}
  end

  defp tune_for_sample_size({weights, notes}, total_records) do
    if total_records < 50 do
      {scale_all(weights, 0.90),
       ["Low sample size (<50 records); keep weights conservative." | notes]}
    else
      {weights, notes}
    end
  end

  defp normalized_distribution(counts) do
    total =
      counts
      |> Map.values()
      |> Enum.map(&integer/1)
      |> Enum.sum()

    if total <= 0 do
      %{}
    else
      counts
      |> Enum.map(fn {key, count} ->
        {to_string(key), integer(count) / total}
      end)
      |> Map.new()
    end
  end

  defp top_counts(counts, n) when is_integer(n) and n > 0 do
    counts
    |> Enum.map(fn {key, value} -> {to_string(key), integer(value)} end)
    |> Enum.sort_by(fn {_k, v} -> -v end)
    |> Enum.take(n)
    |> Enum.map(fn {key, count} -> %{"key" => key, "count" => count} end)
  end

  defp scale_all(weights, factor) do
    weights
    |> Enum.map(fn {key, value} -> {key, bounded_weight(value * factor)} end)
    |> Map.new()
  end

  defp scale_weight(weights, key, factor) do
    Map.update(weights, key, 1.0, fn value -> bounded_weight(value * factor) end)
  end

  defp bounded_weight(value) when value < 0.5, do: 0.5
  defp bounded_weight(value) when value > 2.0, do: 2.0
  defp bounded_weight(value), do: value

  defp integer(value) when is_integer(value), do: value

  defp integer(value) when is_float(value), do: trunc(value)

  defp integer(value) when is_binary(value) do
    case Integer.parse(value) do
      {n, ""} -> n
      _ -> 0
    end
  end

  defp integer(_), do: 0

  defp map(value) when is_map(value), do: value
  defp map(_), do: %{}

  defp ratio(_part, total) when total <= 0, do: 0.0
  defp ratio(part, total), do: part / total
end
